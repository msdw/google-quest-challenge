{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import html\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from math import ceil, floor\n",
    "from sklearn.cluster import KMeans\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "START_TIME = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/sacremoses/sacremoses-master\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses==0.0.35) (1.13.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses==0.0.35) (7.0)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses==0.0.35) (0.14.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from sacremoses==0.0.35) (4.39.0)\r\n",
      "Building wheels for collected packages: sacremoses\r\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=882724 sha256=91da4f9a0c40aff1ba26beb8eaff0106091816c07f189a1899a6af0f321aabe2\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/82/48/4b/05cb49d913a40c9d76f97931cd747d72fb17a77b0f6415cdba\r\n",
      "Successfully built sacremoses\r\n",
      "Installing collected packages: sacremoses\r\n",
      "Successfully installed sacremoses-0.0.35\r\n",
      "CPU times: user 2.53 s, sys: 535 ms, total: 3.07 s\n",
      "Wall time: 35.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip install ../input/sacremoses/sacremoses-master/  \n",
    "sys.path.insert(0, \"../input/transformers/transformers-master/\")\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "MODEL_PATHS = {\n",
    "    'bert-base-uncased': '../input/bertconfigs/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/',\n",
    "    'bert-base-cased':   '../input/bertconfigs/cased_L-12_H-768_A-12/cased_L-12_H-768_A-12/',\n",
    "    'bert-large-uncased': '../input/bertconfigs/uncased_L-24_H-1024_A-16/uncased_L-24_H-1024_A-16/',\n",
    "    'bert-large-cased': '../input/bertconfigs/cased_L-24_H-1024_A-16/cased_L-24_H-1024_A-16/',\n",
    "    'bert-large-uncased-whole-word-masking': '../input/bertconfigs/wwm_uncased_L-24_H-1024_A-16/wwm_uncased_L-24_H-1024_A-16/',\n",
    "    'bert-large-cased-whole-word-masking': '../input/bertconfigs/wwm_cased_L-24_H-1024_A-16/wwm_cased_L-24_H-1024_A-16/',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2019\n",
    "\n",
    "CP_PATH = \"../input/qa-cp/\"\n",
    "DATA_PATH = \"../input/google-quest-challenge/\"\n",
    "\n",
    "sub = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n",
    "TARGETS = list(sub.columns[1:])\n",
    "NUM_TARGETS = len(TARGETS)\n",
    "\n",
    "NUM_WORKERS = 1\n",
    "VAL_BS = 64\n",
    "\n",
    "MAX_LEN_T = 50\n",
    "MAX_LEN_Q = 229\n",
    "MAX_LEN_A = 229\n",
    "MAX_LEN = 512\n",
    "\n",
    "SPECIAL_TOKENS = [f\"[tgt{i}]\" for i in range(len(TARGETS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert-base-chinese',\n",
       " 'bert-large-cased-vocab.txt',\n",
       " 'bert-base-chinese-vocab.txt',\n",
       " 'bert-large-uncased-vocab.txt',\n",
       " 'bert-base-multilingual-uncased',\n",
       " 'bert-base-multilingual-uncased-vocab.txt',\n",
       " 'bert-base-uncased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-large-uncased',\n",
       " 'bert-base-cased',\n",
       " 'bert-base-cased-vocab.txt',\n",
       " 'bert-large-cased',\n",
       " 'bert-base-multilingual-cased-vocab.txt',\n",
       " 'bert-base-uncased-vocab.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../input/pretrained-bert-models-for-pytorch/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def create_emb_list(df, varname):\n",
    "    cat = {\"unknown\": 0}\n",
    "    unique_vals = df[varname].unique()\n",
    "\n",
    "    for i in range(len(unique_vals)):\n",
    "        cat[unique_vals[i]] = i + 1\n",
    "\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "\n",
    "HOST_EMB_LIST = create_emb_list(df_train, \"host\")\n",
    "CAT_EMB_LIST = create_emb_list(df_train, \"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def clean_urls(x):\n",
    "    x = re.sub(r'http\\S+', ' URL ', x)\n",
    "    x = re.sub(r'www\\S+', ' URL ', x)\n",
    "    return re.sub(r'@\\S+', ' USERNAME ', x)\n",
    "\n",
    "\n",
    "def clean_apostrophes(x):\n",
    "    apostrophes = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in apostrophes:\n",
    "        x = re.sub(s, \"'\", x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_latex_tags(text):\n",
    "    text = re.sub('(\\[ math \\]).+(\\[ / math \\])', 'MATHS', text)\n",
    "    text = re.sub('(\\$\\$).+(\\$\\$)', 'MATHS', text)\n",
    "    text = re.sub('(\\$).+(\\$)', 'MATHS', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', \n",
    "          '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0', '\\t', '\\n']\n",
    "\n",
    "def clean_spaces(text):\n",
    "    for space in spaces:\n",
    "        text = text.replace(space, ' ')\n",
    "    text = text.strip()\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_numbers(text):\n",
    "    text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text)\n",
    "    text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text)\n",
    "    text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # text = html.unescape(text)\n",
    "    text = clean_apostrophes(text)\n",
    "    text = clean_urls(text)\n",
    "#     text = clean_numbers(text)\n",
    "    text = clean_latex_tags(text)\n",
    "    text = clean_spaces(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def trim_input(t, q, a, max_len_q=200, max_len_a=200, max_len_t=60):\n",
    "    max_len = max_len_t + max_len_q + max_len_a + 4\n",
    "    len_t, len_q, len_a = len(t), len(q), len(a)\n",
    "        \n",
    "    if max_len_t > len_t:\n",
    "        new_len_t = len_t\n",
    "        max_len_a = max_len_a + floor((max_len_t - len_t)/2)\n",
    "        max_len_q = max_len_q + ceil((max_len_t - len_t)/2)\n",
    "    else:\n",
    "        new_len_t = max_len_t\n",
    "    \n",
    "    if max_len_a > len_a:\n",
    "        new_len_a = len_a \n",
    "        new_len_q = max_len_q + (max_len_a - len_a)\n",
    "    elif max_len_q > len_q:\n",
    "        new_len_a = max_len_a + (max_len_q - len_q)\n",
    "        new_len_q = len_q\n",
    "    else:\n",
    "        new_len_a = max_len_a\n",
    "        new_len_q = max_len_q\n",
    "\n",
    "    return t[:new_len_t], q[:new_len_q], a[:new_len_a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def convert_text(\n",
    "    title, question, answer, transformer, max_len_q=200, max_len_a=200, max_len_t=50,\n",
    "):\n",
    "    max_len = 4 + max_len_q + max_len_a + max_len_t  # with sep tokens\n",
    "\n",
    "    title = transformer.tokenizer.tokenize(title)\n",
    "    question = transformer.tokenizer.tokenize(question)\n",
    "    answer = transformer.tokenizer.tokenize(answer)\n",
    "\n",
    "    tokens_t, tokens_q, tokens_a = trim_input(\n",
    "        title,\n",
    "        question,\n",
    "        answer,\n",
    "        max_len_t=max_len_t,\n",
    "        max_len_q=max_len_q,\n",
    "        max_len_a=max_len_a,\n",
    "    )\n",
    "\n",
    "    tokens = [\"[CLS]\"] + tokens_t + [\"[q]\"] + tokens_q + [\"[a]\"] + tokens_a + [\"[SEP]\"]\n",
    "    question = transformer.tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    segments = (\n",
    "        [0] * (1 + len(tokens_t))\n",
    "        + [1] * (1 + len(tokens_q))\n",
    "        + [2] * (2 + len(tokens_a))\n",
    "    )\n",
    "\n",
    "    padding = [0] * (max_len - len(question))\n",
    "\n",
    "    return question + padding, segments + padding\n",
    "\n",
    "\n",
    "def convert_text_sep(\n",
    "    title,\n",
    "    question,\n",
    "    answer,\n",
    "    transformer,\n",
    "    max_len_q=512,\n",
    "    max_len_a=512,\n",
    "    max_len_t=50,\n",
    "    use_special=True,\n",
    "):\n",
    "\n",
    "    title = transformer.tokenizer.tokenize(title)\n",
    "    question = transformer.tokenizer.tokenize(question)\n",
    "    answer = transformer.tokenizer.tokenize(answer)\n",
    "\n",
    "    tokens_t = title[:max_len_t]\n",
    "\n",
    "    if use_special:\n",
    "        tokens_q = [\"[q]\"] + tokens_t + [\"[SEP]\"] + question\n",
    "        tokens_a = [\"[a]\"] + tokens_t + [\"[SEP]\"] + answer\n",
    "    else:\n",
    "        tokens_q = [\"[CLS]\"] + tokens_t + [\"[SEP]\"] + question\n",
    "        tokens_a = [\"[CLS]\"] + tokens_t + [\"[SEP]\"] + answer\n",
    "\n",
    "    tokens_q = tokens_q[: max_len_q - 1] + [\"[SEP]\"]\n",
    "    tokens_a = tokens_a[: max_len_a - 1] + [\"[SEP]\"]\n",
    "\n",
    "    question = transformer.tokenizer.convert_tokens_to_ids(tokens_q)\n",
    "    answer = transformer.tokenizer.convert_tokens_to_ids(tokens_a)\n",
    "\n",
    "    if use_special:\n",
    "        segments_q = [0] * (1 + len(tokens_t)) + [1] * (\n",
    "            len(question) - (1 + len(tokens_t))\n",
    "        )\n",
    "        segments_a = [0] * (1 + len(tokens_t)) + [2] * (\n",
    "            len(answer) - (1 + len(tokens_t))\n",
    "        )\n",
    "    else:\n",
    "        segments_q = [0] * (1 + len(tokens_t)) + [1] * (\n",
    "            len(question) - (1 + len(tokens_t))\n",
    "        )\n",
    "        segments_a = [0] * (1 + len(tokens_t)) + [1] * (\n",
    "            len(answer) - (1 + len(tokens_t))\n",
    "        )\n",
    "\n",
    "    padding_q = [0] * (max_len_q - len(question))\n",
    "    padding_a = [0] * (max_len_a - len(answer))\n",
    "\n",
    "    return (\n",
    "        question + padding_q,\n",
    "        answer + padding_a,\n",
    "        segments_q + padding_q,\n",
    "        segments_a + padding_a,\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_text_special(\n",
    "    title,\n",
    "    question,\n",
    "    answer,\n",
    "    transformer,\n",
    "    max_len_q=200,\n",
    "    max_len_a=200,\n",
    "    max_len_t=50,\n",
    "    augment=False,\n",
    "    margin=20,\n",
    "):\n",
    "    max_len_q -= len(SPECIAL_TOKENS) - 1\n",
    "    max_len = 33 + max_len_q + max_len_a + max_len_t\n",
    "\n",
    "    title = transformer.tokenizer.tokenize(title)\n",
    "    question = transformer.tokenizer.tokenize(question)\n",
    "    answer = transformer.tokenizer.tokenize(answer)\n",
    "\n",
    "    if augment:\n",
    "        question = augment_text(question, min_len=max_len_q + margin)\n",
    "        answer = augment_text(answer, min_len=max_len_a + margin)\n",
    "\n",
    "    tokens_t, tokens_q, tokens_a = trim_input(\n",
    "        title,\n",
    "        question,\n",
    "        answer,\n",
    "        max_len_t=max_len_t,\n",
    "        max_len_q=max_len_q,\n",
    "        max_len_a=max_len_a,\n",
    "    )\n",
    "\n",
    "    tokens = (\n",
    "        SPECIAL_TOKENS + tokens_t + [\"[q]\"] + tokens_q + [\"[a]\"] + tokens_a + [\"[SEP]\"]\n",
    "    )\n",
    "\n",
    "    question = transformer.tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    segments = (\n",
    "        [0] * (len(SPECIAL_TOKENS) + len(tokens_t))\n",
    "        + [1] * (1 + len(tokens_q))\n",
    "        + [2] * (2 + len(tokens_a))\n",
    "    )\n",
    "\n",
    "    padding = [0] * (max_len - len(question))\n",
    "\n",
    "    return question + padding, segments + padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class QATestDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        transformer,\n",
    "        max_len_q=200,\n",
    "        max_len_a=200,\n",
    "        max_len_t=60,\n",
    "        special=False,\n",
    "        preprocess=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokens = []\n",
    "        self.ids = []\n",
    "\n",
    "        if preprocess:\n",
    "            df = df.copy()\n",
    "            df[\"question_title\"] = df[\"question_title\"].apply(clean_text)\n",
    "            df[\"question_body\"] = df[\"question_body\"].apply(clean_text)\n",
    "            df[\"answer\"] = df[\"answer\"].apply(clean_text)\n",
    "\n",
    "        for title, question, answer in zip(\n",
    "            df[\"question_title\"].values, df[\"question_body\"].values, df[\"answer\"].values\n",
    "        ):\n",
    "            if special:\n",
    "                tokens, idx = convert_text_special(\n",
    "                    title,\n",
    "                    question,\n",
    "                    answer,\n",
    "                    transformer,\n",
    "                    max_len_q=max_len_q,\n",
    "                    max_len_a=max_len_a,\n",
    "                    max_len_t=max_len_t,\n",
    "                )\n",
    "            else:\n",
    "                tokens, idx = convert_text(\n",
    "                    title,\n",
    "                    question,\n",
    "                    answer,\n",
    "                    transformer,\n",
    "                    max_len_q=max_len_q,\n",
    "                    max_len_a=max_len_a,\n",
    "                    max_len_t=max_len_t,\n",
    "                )\n",
    "\n",
    "            self.tokens.append(tokens)\n",
    "            self.ids.append(idx)\n",
    "\n",
    "        self.tokens = np.array(self.tokens)\n",
    "        self.ids = np.array(self.ids)\n",
    "        self.df = df \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def getembed(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        host = HOST_EMB_LIST.get(row.host)\n",
    "        cat = CAT_EMB_LIST.get(row.category)\n",
    "\n",
    "        if host is None:\n",
    "            host = HOST_EMB_LIST.get(\"unknown\")\n",
    "\n",
    "        if cat is None:\n",
    "            cat = CAT_EMB_LIST.get(\"unknown\")\n",
    "            \n",
    "        return host, cat\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        host, cat = self.getembed(idx)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(self.tokens[idx]),\n",
    "            torch.tensor(self.ids[idx]),\n",
    "            torch.tensor(host),\n",
    "            torch.tensor(cat),\n",
    "            torch.tensor(0),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class QATestDatasetSep(Dataset):\n",
    "    \"\"\"\n",
    "    Question and answer are separed\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        transformer,\n",
    "        max_len_q=512,\n",
    "        max_len_a=512,\n",
    "        max_len_t=60,\n",
    "        special=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "\n",
    "        self.tokens_q = []\n",
    "        self.tokens_a = []\n",
    "        self.idxs_q = []\n",
    "        self.idxs_a = []\n",
    "\n",
    "        for title, question, answer in zip(\n",
    "            df[\"question_title\"].values, df[\"question_body\"].values, df[\"answer\"].values\n",
    "        ):\n",
    "\n",
    "            tokens_q, tokens_a, idx_q, idx_a = convert_text_sep(\n",
    "                title,\n",
    "                question,\n",
    "                answer,\n",
    "                transformer,\n",
    "                max_len_q=512,\n",
    "                max_len_a=512,\n",
    "                max_len_t=max_len_t,\n",
    "                use_special=special,\n",
    "            )\n",
    "\n",
    "            self.tokens_q.append(tokens_q)\n",
    "            self.tokens_a.append(tokens_a)\n",
    "            self.idxs_q.append(idx_q)\n",
    "            self.idxs_a.append(idx_a)\n",
    "\n",
    "        self.tokens_q = np.array(self.tokens_q)\n",
    "        self.tokens_a = np.array(self.tokens_a)\n",
    "        self.idxs_q = np.array(self.idxs_q)\n",
    "        self.idxs_a = np.array(self.idxs_a)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.tokens_q[idx]),\n",
    "            torch.tensor(self.tokens_a[idx]),\n",
    "            torch.tensor(self.idxs_q[idx]),\n",
    "            torch.tensor(self.idxs_a[idx]),\n",
    "            torch.tensor(0),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class BertMultiPooler(nn.Module):\n",
    "    def __init__(self, nb_layers=1, input_size=768, nb_ft=768, drop_p=0.1, weights=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.nb_layers = nb_layers\n",
    "        self.input_size = input_size\n",
    "        self.poolers = nn.ModuleList([])\n",
    "        \n",
    "        \n",
    "        for i in range(nb_layers):\n",
    "            pooler = nn.Sequential(\n",
    "                nn.Linear(input_size, nb_ft),\n",
    "                # nn.Dropout(drop_p),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "            if weights is not None:\n",
    "                with torch.no_grad():\n",
    "                    pooler[0].weight = nn.Parameter(weights.clone())\n",
    "                    # print('loaded')\n",
    "            self.poolers.append(pooler)\n",
    "        \n",
    "\n",
    "    def forward(self, hidden_states, idx=0):\n",
    "        bs = hidden_states[0].size()[0]\n",
    "        if type(idx) == int:\n",
    "            idx = torch.tensor([idx] * bs).cuda()\n",
    "\n",
    "        outputs = []\n",
    "        idx = idx.view(-1, 1, 1).repeat(1, 1, self.input_size)\n",
    "\n",
    "        for i, (state) in enumerate(hidden_states[:self.nb_layers]):\n",
    "            token_tensor = state.gather(1, idx).view(bs, -1)\n",
    "\n",
    "            pooled = self.poolers[i](token_tensor)\n",
    "            outputs.append(pooled)\n",
    "\n",
    "        return torch.cat(outputs, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class QA_TransformerSpecialOld(nn.Module):\n",
    "    \"\"\" Individual pooler and logits for each targe \"\"\"\n",
    "\n",
    "    def __init__(self, model, nb_layers=1, pooler_ft=None):\n",
    "        super().__init__()\n",
    "        self.name = model\n",
    "\n",
    "        model_class, tokenizer_class, pretrained_weights = TRANSFORMERS[model]\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(MODEL_PATHS[model])\n",
    "\n",
    "        bert_config = transformers.BertConfig.from_json_file(MODEL_PATHS[model] + 'bert_config.json')\n",
    "        bert_config.num_labels = len(TARGETS)\n",
    "        bert_config.output_hidden_states = True\n",
    "        \n",
    "        self.transformer = transformers.BertModel(bert_config)\n",
    "        \n",
    "        self.nb_features = self.transformer.pooler.dense.out_features\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            self.tokenizer.add_tokens([\"[q]\", \"[a]\"])\n",
    "\n",
    "            w = self.transformer.embeddings.word_embeddings.weight\n",
    "            sep_w = w[102].view(1, -1).detach()\n",
    "            self.transformer.embeddings.word_embeddings = nn.Embedding.from_pretrained(\n",
    "                torch.cat([w, sep_w.clone(), sep_w.clone()])\n",
    "            )\n",
    "\n",
    "            self.tokenizer.add_tokens(SPECIAL_TOKENS)\n",
    "            cls_w = w[101].view(1, -1).detach()\n",
    "\n",
    "            w = self.transformer.embeddings.word_embeddings.weight\n",
    "            self.transformer.embeddings.word_embeddings = nn.Embedding.from_pretrained(\n",
    "                torch.cat([w] + [cls_w.clone() for _ in range(len(SPECIAL_TOKENS))])\n",
    "            )\n",
    "\n",
    "            w = self.transformer.embeddings.token_type_embeddings.weight\n",
    "            self.transformer.embeddings.token_type_embeddings = nn.Embedding.from_pretrained(\n",
    "                torch.cat(\n",
    "                    [\n",
    "                        w[0].view(1, -1).detach().clone(),\n",
    "                        w[0].detach().view(1, -1).clone(),\n",
    "                        w[1].detach().view(1, -1).clone(),\n",
    "                    ],\n",
    "                    0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if pooler_ft is None:\n",
    "            pooler_ft = self.nb_features\n",
    "\n",
    "        self.pooler = nn.ModuleList([])\n",
    "        self.logit = nn.ModuleList([])\n",
    "\n",
    "        for i in range(NUM_TARGETS):\n",
    "            self.pooler.append(\n",
    "                BertMultiPooler(\n",
    "                    nb_layers=nb_layers, input_size=self.nb_features, nb_ft=pooler_ft,\n",
    "                )\n",
    "            )\n",
    "            self.logit.append(\n",
    "                nn.Sequential(nn.Dropout(0.1), nn.Linear(pooler_ft * nb_layers, 1))\n",
    "            )\n",
    "\n",
    "    def forward(self, tokens, token_types, host, cat):\n",
    "\n",
    "        _, _, hidden_states = self.transformer(\n",
    "            tokens, attention_mask=(tokens > 0).long(), token_type_ids=token_types,\n",
    "        )\n",
    "\n",
    "        hidden_states = hidden_states[::-1]\n",
    "\n",
    "        pooled = [self.pooler[i](hidden_states, idx=i) for i in range(NUM_TARGETS)]\n",
    "        outputs = [self.logit[i](pooled[i]) for i in range(NUM_TARGETS)]\n",
    "\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class QA_TransformerOld(nn.Module):\n",
    "    def __init__(self, model, nb_layers=1, pooler_ft=None, use_special_tokens=False):\n",
    "        super().__init__()\n",
    "        self.name = model\n",
    "        ref = None\n",
    "        self.use_special_tokens = use_special_tokens\n",
    "\n",
    "        model_class, tokenizer_class, pretrained_weights = TRANSFORMERS[model]\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(MODEL_PATHS[model])\n",
    "\n",
    "        bert_config = transformers.BertConfig.from_json_file(MODEL_PATHS[model] + 'bert_config.json')\n",
    "        bert_config.num_labels = len(TARGETS)\n",
    "        bert_config.output_hidden_states = True\n",
    "        \n",
    "        self.transformer = transformers.BertModel(bert_config)\n",
    "        \n",
    "        self.nb_features = self.transformer.pooler.dense.out_features\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.tokenizer.add_tokens([\"[q]\", \"[a]\"])\n",
    "\n",
    "            w = self.transformer.embeddings.word_embeddings.weight\n",
    "            sep_w = w[102].view(1, -1).detach()\n",
    "            self.transformer.embeddings.word_embeddings = nn.Embedding.from_pretrained(\n",
    "                torch.cat([w, sep_w.clone(), sep_w.clone()])\n",
    "            )\n",
    "\n",
    "            w = self.transformer.embeddings.token_type_embeddings.weight\n",
    "            self.transformer.embeddings.token_type_embeddings = nn.Embedding.from_pretrained(\n",
    "                torch.cat(\n",
    "                    [\n",
    "                        w[0].view(1, -1).detach().clone(),\n",
    "                        w[0].detach().view(1, -1).clone(),\n",
    "                        w[1].detach().view(1, -1).clone(),\n",
    "                    ],\n",
    "                    0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if pooler_ft is None or pooler_ft == self.nb_features:\n",
    "            pooler_ft = self.nb_features\n",
    "            ref = self.transformer.pooler.dense.weight.detach().clone()\n",
    "\n",
    "        self.pooler_all = BertMultiPooler(\n",
    "            nb_layers=nb_layers,\n",
    "            input_size=self.nb_features,\n",
    "            nb_ft=pooler_ft,\n",
    "            weights=ref,\n",
    "        )\n",
    "\n",
    "        if self.use_special_tokens:  # Use features corresponding to [q] and [a] tokens\n",
    "            self.pooler_q = BertMultiPooler(\n",
    "                nb_layers=nb_layers,\n",
    "                input_size=self.nb_features,\n",
    "                nb_ft=pooler_ft,\n",
    "                weights=ref,\n",
    "            )\n",
    "\n",
    "            self.pooler_a = BertMultiPooler(\n",
    "                nb_layers=nb_layers,\n",
    "                input_size=self.nb_features,\n",
    "                nb_ft=pooler_ft,\n",
    "                weights=ref,\n",
    "            )\n",
    "\n",
    "            self.logit = nn.Linear(3 * pooler_ft * nb_layers, len(TARGETS))\n",
    "\n",
    "        else:\n",
    "            self.logit = nn.Linear(pooler_ft * nb_layers, len(TARGETS))\n",
    "\n",
    "    def forward(self, tokens, token_types, host, cat):\n",
    "\n",
    "        _, _, hidden_states = self.transformer(\n",
    "            tokens, attention_mask=(tokens > 0).long(), token_type_ids=token_types,\n",
    "        )\n",
    "\n",
    "        hidden_states = hidden_states[::-1]\n",
    "\n",
    "        ft = self.pooler_all(hidden_states, 0)\n",
    "\n",
    "        if self.use_special_tokens:\n",
    "            ft_q = self.pooler_q(hidden_states, q_idx)\n",
    "            ft_a = self.pooler_a(hidden_states, a_idx)\n",
    "            ft = torch.cat([ft, ft_q, ft_a], 1)\n",
    "\n",
    "        return self.logit(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class QA_TransformerMixOld(nn.Module):\n",
    "    def __init__(self, model, nb_layers=1, pooler_ft=None, use_special_tokens=False):\n",
    "        super().__init__()\n",
    "        self.name = model\n",
    "        ref = None\n",
    "        self.use_special_tokens = use_special_tokens\n",
    "\n",
    "        model_class, tokenizer_class, pretrained_weights = TRANSFORMERS[model]\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(MODEL_PATHS[model])\n",
    "\n",
    "        bert_config = transformers.BertConfig.from_json_file(MODEL_PATHS[model] + 'bert_config.json')\n",
    "        bert_config.num_labels = len(TARGETS)\n",
    "        bert_config.output_hidden_states = True\n",
    "        \n",
    "        self.transformer = transformers.BertModel(bert_config)\n",
    "        \n",
    "        self.nb_features = self.transformer.pooler.dense.out_features\n",
    "\n",
    "        if pooler_ft is None:\n",
    "            pooler_ft = self.nb_features\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.tokenizer.add_tokens([\"[q]\", \"[a]\"])\n",
    "\n",
    "            w = self.transformer.embeddings.word_embeddings.weight\n",
    "            sep_w = w[101].view(1, -1).detach()\n",
    "            self.transformer.embeddings.word_embeddings = nn.Embedding.from_pretrained(\n",
    "                torch.cat([w, sep_w.clone(), sep_w.clone()])\n",
    "            )\n",
    "\n",
    "            w = self.transformer.embeddings.token_type_embeddings.weight\n",
    "            self.transformer.embeddings.token_type_embeddings = nn.Embedding.from_pretrained(\n",
    "                torch.cat(\n",
    "                    [\n",
    "                        w[0].detach().view(1, -1).clone(),\n",
    "                        w[0].detach().view(1, -1).clone(),\n",
    "                        w[1].detach().view(1, -1).clone(),\n",
    "                    ],\n",
    "                    0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.pooler_a = BertMultiPooler(\n",
    "            nb_layers=nb_layers,\n",
    "            input_size=self.nb_features,\n",
    "            nb_ft=pooler_ft,\n",
    "        )\n",
    "\n",
    "        self.pooler_q = BertMultiPooler(\n",
    "            nb_layers=nb_layers,\n",
    "            input_size=self.nb_features,\n",
    "            nb_ft=pooler_ft,\n",
    "        )\n",
    "        \n",
    "        # 0 = both, 1 = q, 2 = a\n",
    "        self.mix = [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2]\n",
    "        assert len(self.mix) == len(TARGETS)\n",
    "\n",
    "        self.logit = nn.ModuleList([])\n",
    "\n",
    "        for i in range(len(self.mix)):\n",
    "            self.logit.append(nn.Linear((2 - (self.mix[i] > 0)) * pooler_ft * nb_layers, 1))\n",
    "\n",
    "    def forward(self, tokens_q, tokens_a, token_types_q, token_types_a):\n",
    "\n",
    "        _, _, hidden_states_q = self.transformer(\n",
    "            tokens_q, attention_mask=(tokens_q > 0).long(), token_type_ids=token_types_q,\n",
    "        )\n",
    "\n",
    "        _, _, hidden_states_a = self.transformer(\n",
    "            tokens_a, attention_mask=(tokens_a > 0).long(), token_type_ids=token_types_a,\n",
    "        )\n",
    "\n",
    "        hidden_states_q = hidden_states_q[::-1]\n",
    "        hidden_states_a = hidden_states_a[::-1]\n",
    "\n",
    "        ft_q = self.pooler_q(hidden_states_q, 0)\n",
    "        ft_a = self.pooler_a(hidden_states_a, 0)\n",
    "\n",
    "        outs = []\n",
    "\n",
    "        for i in range(len(self.mix)):\n",
    "            if self.mix[i] == 0:\n",
    "                outs.append(self.logit[i](torch.cat([ft_q, ft_a], -1)))\n",
    "            elif self.mix[i] == 1:\n",
    "                outs.append(self.logit[i](ft_q))\n",
    "            else:\n",
    "                outs.append(self.logit[i](ft_a))\n",
    "\n",
    "        return torch.cat(outs, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class QA_TransformerMix(nn.Module):\n",
    "    def __init__(self, model, nb_layers=1, pooler_ft=None, use_special_tokens=False):\n",
    "        super().__init__()\n",
    "        self.name = model\n",
    "        self.use_special_tokens = use_special_tokens\n",
    "\n",
    "        model_class, tokenizer_class, pretrained_weights = TRANSFORMERS[model]\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(MODEL_PATHS[model])\n",
    "\n",
    "        bert_config = transformers.BertConfig.from_json_file(MODEL_PATHS[model] + 'bert_config.json')\n",
    "        bert_config.num_labels = len(TARGETS)\n",
    "        bert_config.output_hidden_states = True\n",
    "        \n",
    "        self.transformer = transformers.BertModel(bert_config)\n",
    "        \n",
    "        self.nb_features = self.transformer.pooler.dense.out_features\n",
    "\n",
    "        if pooler_ft is None:\n",
    "            pooler_ft = self.nb_features\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.tokenizer.add_tokens([\"[q]\", \"[a]\"])\n",
    "\n",
    "            w = self.transformer.embeddings.word_embeddings.weight\n",
    "            sep_w = w[101].view(1, -1).detach()\n",
    "            self.transformer.embeddings.word_embeddings = nn.Embedding.from_pretrained(\n",
    "                torch.cat([w, sep_w.clone(), sep_w.clone()])\n",
    "            )\n",
    "\n",
    "            w = self.transformer.embeddings.token_type_embeddings.weight\n",
    "            self.transformer.embeddings.token_type_embeddings = nn.Embedding.from_pretrained(\n",
    "                torch.cat(\n",
    "                    [\n",
    "                        w[0].detach().view(1, -1).clone(),\n",
    "                        w[0].detach().view(1, -1).clone(),\n",
    "                        w[1].detach().view(1, -1).clone(),\n",
    "                    ],\n",
    "                    0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.pooler_a = BertMultiPooler(\n",
    "            nb_layers=nb_layers, input_size=self.nb_features, nb_ft=pooler_ft,\n",
    "        )\n",
    "\n",
    "        self.pooler_q = BertMultiPooler(\n",
    "            nb_layers=nb_layers, input_size=self.nb_features, nb_ft=pooler_ft,\n",
    "        )\n",
    "\n",
    "        # 0 = both, 1 = q, 2 = a   > I have to work on this\n",
    "        self.mix = [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2]\n",
    "        assert len(self.mix) == len(TARGETS)\n",
    "\n",
    "        self.logit = nn.ModuleList([])\n",
    "\n",
    "        for i in range(len(self.mix)):\n",
    "            self.logit.append(\n",
    "                nn.Linear((2 - (self.mix[i] > 0)) * pooler_ft * nb_layers, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, tokens_q, tokens_a, token_types_q, token_types_a):\n",
    "\n",
    "        _, _, hidden_states_q = self.transformer(\n",
    "            tokens_q,\n",
    "            attention_mask=(tokens_q > 0).long(),\n",
    "            token_type_ids=token_types_q,\n",
    "        )\n",
    "\n",
    "        _, _, hidden_states_a = self.transformer(\n",
    "            tokens_a,\n",
    "            attention_mask=(tokens_a > 0).long(),\n",
    "            token_type_ids=token_types_a,\n",
    "        )\n",
    "\n",
    "        hidden_states_q = hidden_states_q[::-1]\n",
    "        hidden_states_a = hidden_states_a[::-1]\n",
    "\n",
    "        ft_q = self.pooler_q(hidden_states_q, 0)\n",
    "        ft_a = self.pooler_a(hidden_states_a, 0)\n",
    "\n",
    "        outs = []\n",
    "\n",
    "        for i in range(len(self.mix)):\n",
    "            if self.mix[i] == 0:\n",
    "                outs.append(self.logit[i](torch.cat([ft_q, ft_a], -1)))\n",
    "            elif self.mix[i] == 1:\n",
    "                outs.append(self.logit[i](ft_q))\n",
    "            else:\n",
    "                outs.append(self.logit[i](ft_a))\n",
    "\n",
    "        return torch.cat(outs, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class QA_TransformerFt(nn.Module):\n",
    "    def __init__(self, model, nb_layers=1, pooler_ft=None, use_special_tokens=False):\n",
    "        super().__init__()\n",
    "        self.name = model\n",
    "\n",
    "        model_class, tokenizer_class, pretrained_weights = TRANSFORMERS[model]\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(MODEL_PATHS[model])\n",
    "\n",
    "        bert_config = transformers.BertConfig.from_json_file(MODEL_PATHS[model] + 'bert_config.json')\n",
    "        bert_config.num_labels = len(TARGETS)\n",
    "        bert_config.output_hidden_states = True\n",
    "        \n",
    "        self.transformer = transformers.BertModel(bert_config)\n",
    "        \n",
    "        self.nb_features = self.transformer.pooler.dense.out_features\n",
    "        \n",
    "        if pooler_ft is None:\n",
    "            pooler_ft = self.nb_features\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.tokenizer.add_tokens([\"[q]\", \"[a]\"])\n",
    "\n",
    "            w = self.transformer.embeddings.word_embeddings.weight\n",
    "            sep_w = w[102].view(1, -1).detach()\n",
    "            self.transformer.embeddings.word_embeddings = nn.Embedding.from_pretrained(\n",
    "                torch.cat([w, sep_w.clone(), sep_w.clone()])\n",
    "            )\n",
    "\n",
    "            w = self.transformer.embeddings.token_type_embeddings.weight\n",
    "            self.transformer.embeddings.token_type_embeddings = nn.Embedding.from_pretrained(\n",
    "                torch.cat(\n",
    "                    [\n",
    "                        w[0].view(1, -1).detach().clone(),\n",
    "                        w[0].detach().view(1, -1).clone(),\n",
    "                        w[1].detach().view(1, -1).clone(),\n",
    "                    ],\n",
    "                    0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.pooler_all = BertMultiPooler(\n",
    "            nb_layers=nb_layers,\n",
    "            input_size=self.nb_features,\n",
    "            nb_ft=pooler_ft,\n",
    "        )\n",
    "\n",
    "        cat_ft = 64\n",
    "        host_ft = 64\n",
    "\n",
    "        self.host_emb = nn.Embedding(64, host_ft)\n",
    "        self.cat_emb = nn.Embedding(6, cat_ft)\n",
    "\n",
    "        self.logit = nn.Linear(pooler_ft * nb_layers + host_ft + cat_ft, len(TARGETS))\n",
    "\n",
    "    def forward(self, tokens, token_types, host, cat):\n",
    "\n",
    "        _, _, hidden_states = self.transformer(\n",
    "            tokens, attention_mask=(tokens > 0).long(), token_type_ids=token_types,\n",
    "        )\n",
    "\n",
    "        hidden_states = hidden_states[::-1]\n",
    "\n",
    "        ft = self.pooler_all(hidden_states, 0)\n",
    "\n",
    "        cat_emb = torch.tanh(self.cat_emb(cat))\n",
    "        host_emb = torch.tanh(self.host_emb(host))\n",
    "\n",
    "        ft = torch.cat((ft, cat_emb, host_emb), 1)\n",
    "\n",
    "        return self.logit(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class QA_TransformerSpecial(nn.Module):\n",
    "    \"\"\" Individual pooler and logits for each targe \"\"\"\n",
    "\n",
    "    def __init__(self, model, nb_layers=1, pooler_ft=None):\n",
    "        super().__init__()\n",
    "        self.name = model\n",
    "\n",
    "        model_class, tokenizer_class, pretrained_weights = TRANSFORMERS[model]\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(MODEL_PATHS[model])\n",
    "\n",
    "        bert_config = transformers.BertConfig.from_json_file(MODEL_PATHS[model] + 'bert_config.json')\n",
    "        bert_config.num_labels = len(TARGETS)\n",
    "        bert_config.output_hidden_states = True\n",
    "        \n",
    "        self.transformer = transformers.BertModel(bert_config)\n",
    "        \n",
    "        self.nb_features = self.transformer.pooler.dense.out_features\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.tokenizer.add_tokens([\"[q]\", \"[a]\"])\n",
    "\n",
    "            w = self.transformer.embeddings.word_embeddings.weight\n",
    "            sep_w = w[102].view(1, -1).detach()\n",
    "            self.transformer.embeddings.word_embeddings = nn.Embedding.from_pretrained(\n",
    "                torch.cat([w, sep_w.clone(), sep_w.clone()])\n",
    "            )\n",
    "\n",
    "            self.tokenizer.add_tokens(SPECIAL_TOKENS)\n",
    "            cls_w = w[101].view(1, -1).detach()\n",
    "\n",
    "            w = self.transformer.embeddings.word_embeddings.weight\n",
    "            self.transformer.embeddings.word_embeddings = nn.Embedding.from_pretrained(\n",
    "                torch.cat([w] + [cls_w.clone() for _ in range(len(SPECIAL_TOKENS))])\n",
    "            )\n",
    "\n",
    "            w = self.transformer.embeddings.token_type_embeddings.weight\n",
    "            self.transformer.embeddings.token_type_embeddings = nn.Embedding.from_pretrained(\n",
    "                torch.cat(\n",
    "                    [\n",
    "                        w[0].view(1, -1).detach().clone(),\n",
    "                        w[0].detach().view(1, -1).clone(),\n",
    "                        w[1].detach().view(1, -1).clone(),\n",
    "                    ],\n",
    "                    0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if pooler_ft is None:\n",
    "            pooler_ft = self.nb_features\n",
    "\n",
    "        self.pooler = nn.ModuleList([])\n",
    "        self.logit = nn.ModuleList([])\n",
    "\n",
    "        for i in range(NUM_TARGETS):\n",
    "            self.pooler.append(\n",
    "                BertMultiPooler(\n",
    "                    nb_layers=nb_layers, input_size=self.nb_features, nb_ft=pooler_ft,\n",
    "                )\n",
    "            )\n",
    "            self.logit.append(\n",
    "                nn.Sequential(nn.Dropout(0.1), nn.Linear(pooler_ft * nb_layers, 1))\n",
    "            )\n",
    "    \n",
    "    def forward(self, tokens, token_types, host, cat):\n",
    "        _, _, hidden_states = self.transformer(\n",
    "            tokens, attention_mask=(tokens > 0).long(), token_type_ids=token_types,\n",
    "        )\n",
    "\n",
    "        hidden_states = hidden_states[::-1]\n",
    "\n",
    "        pooled = [self.pooler[i](hidden_states, idx=i) for i in range(NUM_TARGETS)]\n",
    "        outputs = [self.logit[i](pooled[i]) for i in range(NUM_TARGETS)]\n",
    "\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA_TransformerDouble(nn.Module):\n",
    "    def __init__(self, model, nb_layers=1, pooler_ft=None, use_special_tokens=False):\n",
    "        super().__init__()\n",
    "        self.name = model\n",
    "        ref = None\n",
    "        self.use_special_tokens = use_special_tokens\n",
    "\n",
    "        model_class, tokenizer_class, pretrained_weights = TRANSFORMERS[model]\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(MODEL_PATHS[model])\n",
    "\n",
    "        bert_config = transformers.BertConfig.from_json_file(MODEL_PATHS[model] + 'bert_config.json')\n",
    "        bert_config.num_labels = len(TARGETS)\n",
    "        bert_config.output_hidden_states = True\n",
    "        \n",
    "        self.transformer = transformers.BertModel(bert_config)\n",
    "        self.transformer2 = transformers.BertModel(bert_config)\n",
    "\n",
    "        self.nb_features = self.transformer.pooler.dense.out_features\n",
    "\n",
    "        if pooler_ft is None:\n",
    "            pooler_ft = self.nb_features\n",
    "\n",
    "        self.pooler_a = BertMultiPooler(\n",
    "            nb_layers=nb_layers, input_size=self.nb_features, nb_ft=pooler_ft,\n",
    "        )\n",
    "\n",
    "        self.pooler_q = BertMultiPooler(\n",
    "            nb_layers=nb_layers, input_size=self.nb_features, nb_ft=pooler_ft,\n",
    "        )\n",
    "\n",
    "        # 0 = both, 1 = q, 2 = a\n",
    "        # self.mix = [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2]\n",
    "        # self.mix = [1] * 21 + [2] * 9\n",
    "        self.mix = [0] * 30 #+ [2] * 9\n",
    "\n",
    "        assert len(self.mix) == len(TARGETS)\n",
    "\n",
    "        self.logit = nn.ModuleList([])\n",
    "\n",
    "        for i in range(len(self.mix)):\n",
    "            self.logit.append(\n",
    "                nn.Linear((2 - (self.mix[i] > 0)) * pooler_ft * nb_layers, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, tokens_q, tokens_a, token_types_q, token_types_a):\n",
    "\n",
    "        _, _, hidden_states_q = self.transformer(\n",
    "            tokens_q,\n",
    "            attention_mask=(tokens_q > 0).long(),\n",
    "            token_type_ids=token_types_q,\n",
    "        )\n",
    "\n",
    "        _, _, hidden_states_a = self.transformer2(\n",
    "            tokens_a,\n",
    "            attention_mask=(tokens_a > 0).long(),\n",
    "            token_type_ids=token_types_a,\n",
    "        )\n",
    "\n",
    "        hidden_states_q = hidden_states_q[::-1]\n",
    "        hidden_states_a = hidden_states_a[::-1]\n",
    "\n",
    "        ft_q = self.pooler_q(hidden_states_q, 0)\n",
    "        ft_a = self.pooler_a(hidden_states_a, 0)\n",
    "\n",
    "        outs = []\n",
    "\n",
    "        for i in range(len(self.mix)):\n",
    "            if self.mix[i] == 0:\n",
    "                outs.append(self.logit[i](torch.cat([ft_q, ft_a], -1)))\n",
    "            elif self.mix[i] == 1:\n",
    "                outs.append(self.logit[i](ft_q))\n",
    "            else:\n",
    "                outs.append(self.logit[i](ft_a))\n",
    "\n",
    "        return torch.cat(outs, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def load_model_weights(model, filename, verbose=1, cp_folder='', strict=True):\n",
    "    if verbose:\n",
    "        print(f'\\n -> Loading weights from {os.path.join(cp_folder,filename)}\\n')\n",
    "    try:\n",
    "        model.load_state_dict(os.path.join(cp_folder, filename), strict=strict)\n",
    "    except BaseException:\n",
    "        model.load_state_dict(torch.load(os.path.join(cp_folder, filename), map_location='cpu'), strict=strict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def predict(model, dataset, batch_size=8, sep=False, num_workers=1):\n",
    "    model.eval()\n",
    "    preds = np.empty((0, NUM_TARGETS))\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            if sep:\n",
    "                tokens_q, tokens_a, idx_q, idx_a, y_batch = data\n",
    "                y_pred = model(tokens_q.cuda(), tokens_a.cuda(), idx_q.cuda(), idx_a.cuda()).detach()\n",
    "            else:\n",
    "                tokens, idx, host, cat, y_batch = data\n",
    "                y_pred = model(tokens.cuda(), idx.cuda(), host.cuda(), cat.cuda()).detach()\n",
    "\n",
    "            preds = np.concatenate([preds, torch.sigmoid(y_pred).cpu().numpy()])\n",
    "\n",
    "    del y_pred, y_batch, loader\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def inference(model, dataset, weights_folder, sep=False, batch_size=VAL_BS, num_workers=NUM_WORKERS):\n",
    "    pred_test = np.zeros((len(dataset), NUM_TARGETS))\n",
    "    \n",
    "    cps = [f for f in os.listdir(weights_folder) if '.pt' in f or '.bin' in f]\n",
    "    assert len(cps) == 5\n",
    "    \n",
    "    for weight in cps:\n",
    "        model = load_model_weights(model, weight, cp_folder=weights_folder) \n",
    "        pred_test += predict(model, dataset, sep=sep, batch_size=batch_size, num_workers=num_workers) / len(cps)\n",
    "        \n",
    "    return pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "TRANSFORMERS  = {\n",
    "    'bert-base-uncased' :  (transformers.BertModel,       transformers.BertTokenizer,       'bert-base-uncased'),\n",
    "    'bert-base-cased' :    (transformers.BertModel,       transformers.BertTokenizer,       'bert-base-cased'),\n",
    "    'bert-large-uncased' : (transformers.BertModel,       transformers.BertTokenizer,       'bert-large-uncased'),\n",
    "    'bert-large-cased' :   (transformers.BertModel,       transformers.BertTokenizer,       'bert-large-cased'),\n",
    "    'bert-large-uncased-whole-word-masking': (transformers.BertModel,       transformers.BertTokenizer,       'bert-large-uncased-whole-word-masking'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(DATA_PATH + \"test.csv\")\n",
    "pred_tests = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Base - QA_TransformerOld\n",
    "> '28_01': '../output/pred_oof2020-01-28_0.405.npy',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Loading weights from ../input/specialdataset/0.405/2020-01-23/bert-base-uncased_2_cp.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/specialdataset/0.405/2020-01-23/bert-base-uncased_3_cp.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/specialdataset/0.405/2020-01-23/bert-base-uncased_5_cp.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/specialdataset/0.405/2020-01-23/bert-base-uncased_1_cp.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/specialdataset/0.405/2020-01-23/bert-base-uncased_4_cp.pt\n",
      "\n",
      "CPU times: user 36.8 s, sys: 21.1 s, total: 57.8 s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = QA_TransformerOld('bert-base-uncased', nb_layers=8, pooler_ft=1024).cuda()\n",
    "test_dataset = QATestDataset(df_test, model, max_len_q=MAX_LEN_Q, max_len_a=MAX_LEN_A, max_len_t=MAX_LEN_T)\n",
    "\n",
    "pred_test = inference(model, test_dataset, '../input/specialdataset/0.405/2020-01-23/')\n",
    "pred_tests.append(pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Base -  QA_TransformerSpecialOld\n",
    ">'25_01': '../output/pred_oof2020-01-25_0.406.npy',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# model = QA_TransformerSpecialOld('bert-base-uncased', nb_layers=8, pooler_ft=128).cuda()\n",
    "# test_dataset_special = QATestDataset(df_test, model, max_len_q=MAX_LEN_Q, max_len_a=MAX_LEN_A, max_len_t=MAX_LEN_T, special=True)\n",
    "\n",
    "# pred_test = inference(model, test_dataset_special, '../input/specialdataset/0.4447/2020-01-28/')\n",
    "# pred_tests.append(pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Base - QA_TransformerMixOld\n",
    "> '01_02': '../output/pred_oof2020-02-01_0.411.npy',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# model = QA_TransformerMixOld('bert-base-uncased', nb_layers=8, pooler_ft=512).cuda()\n",
    "# test_dataset_sep = QATestDatasetSep(df_test, model, max_len_q=MAX_LEN, max_len_a=MAX_LEN, max_len_t=MAX_LEN_T, special=True)\n",
    "\n",
    "# pred_test = inference(model, test_dataset_sep, '../input/specialdataset/2020-02-01/2020-02-01/', sep=True)\n",
    "# pred_tests.append(pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Base - QA_TransformerFt\n",
    "> 'embed': '../output/pred_oof2020-02-06_0.4089.npy',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# model = QA_TransformerFt('bert-base-uncased', nb_layers=8, pooler_ft=1024).cuda()\n",
    "# test_dataset_pp = QATestDataset(df_test, model, max_len_q=MAX_LEN_Q, max_len_a=MAX_LEN_A, max_len_t=MAX_LEN_T, preprocess=True)\n",
    "\n",
    "# pred_test = inference(model, test_dataset_pp, '../input/bertbase06022/2020-02-06/')\n",
    "# pred_tests.append(pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Base - QA_TransformerFt Weighted BCE\n",
    "> 'embedw' : '../output/pred_oof2020-02-07_0.4088.npy',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Loading weights from ../input/qa-bert-weightedbce/2020-02-07/bert-base-uncased_2_cp.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/qa-bert-weightedbce/2020-02-07/bert-base-uncased_3_cp.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/qa-bert-weightedbce/2020-02-07/bert-base-uncased_5_cp.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/qa-bert-weightedbce/2020-02-07/bert-base-uncased_1_cp.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/qa-bert-weightedbce/2020-02-07/bert-base-uncased_4_cp.pt\n",
      "\n",
      "CPU times: user 32.6 s, sys: 19.4 s, total: 52 s\n",
      "Wall time: 54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = QA_TransformerFt('bert-base-uncased', nb_layers=8, pooler_ft=1024).cuda()\n",
    "test_dataset_pp = QATestDataset(df_test, model, max_len_q=MAX_LEN_Q, max_len_a=MAX_LEN_A, max_len_t=MAX_LEN_T, preprocess=True)\n",
    "\n",
    "pred_test = inference(model, test_dataset_pp, '../input/qa-bert-weightedbce/2020-02-07/')\n",
    "pred_tests.append(pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Base - QA_TransformerSpecial\n",
    "> 'special':   '../output/pred_oof2020-02-06_0.4059.npy',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# model = QA_TransformerSpecial('bert-base-uncased', nb_layers=8, pooler_ft=128).cuda()\n",
    "# test_dataset_special = QATestDataset(df_test, model, max_len_q=MAX_LEN_Q, max_len_a=MAX_LEN_A, max_len_t=MAX_LEN_T, special=True)\n",
    "\n",
    "# pred_test = inference(model, test_dataset_special, '../input/bertbase0602/0.4059/2020-02-06/')\n",
    "# pred_tests.append(pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Base - QA_TransformerSpecial Weighted BCE\n",
    "> 'specialw' : '../output/pred_oof2020-02-07_0.4058.npy',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# model = QA_TransformerSpecial('bert-base-uncased', nb_layers=8, pooler_ft=128).cuda()\n",
    "# test_dataset_special = QATestDataset(df_test, model, max_len_q=MAX_LEN_Q, max_len_a=MAX_LEN_A, max_len_t=MAX_LEN_T, special=True)\n",
    "\n",
    "# pred_test = inference(model, test_dataset_special, '../input/bertbase06022/0.4058/2020-02-08/')\n",
    "# pred_tests.append(pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Base - QA_TransformerMix\n",
    "> 'mix':     '../output/pred_oof2020-02-06_0.414.npy',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# model = QA_TransformerMix('bert-base-uncased', nb_layers=8, pooler_ft=512).cuda()\n",
    "# test_dataset_sep = QATestDatasetSep(df_test, model, max_len_q=MAX_LEN, max_len_a=MAX_LEN, max_len_t=MAX_LEN_T, special=True)\n",
    "\n",
    "# pred_test = inference(model, test_dataset_sep, '../input/bertbase0602/0.414/2020-02-05/', sep=True)\n",
    "# pred_tests.append(pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Base - QA_TransformerMix Weighted BCE\n",
    "> 'mixw' : '../output/pred_oof2020-02-07_0.414-wbce.npy',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# model = QA_TransformerMix('bert-base-uncased', nb_layers=8, pooler_ft=512).cuda()\n",
    "# test_dataset_sep = QATestDatasetSep(df_test, model, max_len_q=MAX_LEN, max_len_a=MAX_LEN, max_len_t=MAX_LEN_T, special=True)\n",
    "\n",
    "# pred_test = inference(model, test_dataset_sep, '../input/weightedbce-oof-04465/', sep=True)\n",
    "# pred_tests.append(pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Base - QA_TransformerDouble Weighted BCE\n",
    "> 'double': '../output/pred_oof2020-02-07_0.4111.npy', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# model = QA_TransformerDouble('bert-base-uncased', nb_layers=8, pooler_ft=512).cuda()\n",
    "# test_dataset_double = QATestDatasetSep(df_test, model, max_len_q=MAX_LEN, max_len_a=MAX_LEN, max_len_t=MAX_LEN_T, special=False)\n",
    "\n",
    "# pred_test = inference(model, test_dataset_double, '../input/qadoubleweighted/2020-02-07/', sep=True)\n",
    "# pred_tests.append(pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Large WWM - QA_TransformerMix\n",
    "> 'wwm' : '../output/pred_oof2020-02-06_0.416-wwm.npy',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "\n",
    "# model = QA_TransformerMix('bert-large-uncased-whole-word-masking', nb_layers=8, pooler_ft=512).cuda()\n",
    "# test_dataset_sep_wwm = QATestDatasetSep(df_test, model, max_len_q=MAX_LEN, max_len_a=MAX_LEN, max_len_t=MAX_LEN_T, special=True)\n",
    "\n",
    "# pred_test = inference(model, test_dataset_sep_wwm, '../input/questqabertlarge/bert-large-custom/', sep=True)\n",
    "# pred_tests.append(pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Large WWM - QA_TransformerMix 2\n",
    "> 'wwm2' : '../output/pred_oof2020-02-08_0.416_wwm_v2.npy',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# model = QA_TransformerMix('bert-large-uncased-whole-word-masking', nb_layers=8, pooler_ft=768).cuda()\n",
    "# test_dataset_sep_wwm = QATestDatasetSep(df_test, model, max_len_q=MAX_LEN, max_len_a=MAX_LEN, max_len_t=MAX_LEN_T, special=True)\n",
    "\n",
    "# pred_test = inference(model, test_dataset_sep_wwm, '../input/questqabertlargev2/bert-large-custom/', sep=True)\n",
    "# pred_tests.append(pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Large WWM - QA_TransformerMix Weighted BCE\n",
    "> 'wwm3': '../input/questqabertlargev3/pred_oof2020-02-08_0.415.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Loading weights from ../input/questqabertlargev3/bert-large-custom/bert-large-custom/bert_pytorch_fold_1.bin\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/questqabertlargev3/bert-large-custom/bert-large-custom/bert_pytorch_fold_4.bin\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/questqabertlargev3/bert-large-custom/bert-large-custom/bert_pytorch_fold_2.bin\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/questqabertlargev3/bert-large-custom/bert-large-custom/bert_pytorch_fold_0.bin\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/questqabertlargev3/bert-large-custom/bert-large-custom/bert_pytorch_fold_3.bin\n",
      "\n",
      "CPU times: user 2min 42s, sys: 1min 56s, total: 4min 39s\n",
      "Wall time: 4min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = QA_TransformerMix('bert-large-uncased-whole-word-masking', nb_layers=8, pooler_ft=768).cuda()\n",
    "test_dataset_sep_wwm = QATestDatasetSep(df_test, model, max_len_q=MAX_LEN, max_len_a=MAX_LEN, max_len_t=MAX_LEN_T, special=True)\n",
    "\n",
    "pred_test = inference(model, test_dataset_sep_wwm, '../input/questqabertlargev3/bert-large-custom/bert-large-custom/', sep=True)\n",
    "pred_tests.append(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_spelling = pred_test[:, 19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Large Cased\n",
    "> 'large1': '../output/pred_oof2020-02-07_0.416-bl1.npy',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Loading weights from ../input/bertlarge1/bert-large-cased_4_cp.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/bertlarge1/bert-large-cased_1_cp.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/bertlarge1/bert-large-cased_3_cp.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/bertlarge1/bert-large-cased_5_cp.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/bertlarge1/bert-large-cased_2_cp.pt\n",
      "\n",
      "CPU times: user 2min 43s, sys: 1min 57s, total: 4min 40s\n",
      "Wall time: 4min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = QA_TransformerMix('bert-large-cased', nb_layers=24, pooler_ft=128).cuda()\n",
    "test_dataset_sep_blc = QATestDatasetSep(df_test, model, max_len_q=MAX_LEN, max_len_a=MAX_LEN, max_len_t=MAX_LEN_T, special=True)\n",
    "\n",
    "pred_test = inference(model, test_dataset_sep_blc, '../input/bertlarge1/', sep=True)\n",
    "pred_tests.append(pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Large Uncased\n",
    ">  'large3': '../output/pred_oof2020-02-08_0.417-bl3.npy',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Loading weights from ../input/bertlarge3/bert-large-uncased_5_cp.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/bertlarge3/bert-large-uncased_1_cp.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/bertlarge3/bert-large-uncased_4_cp.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/bertlarge3/bert-large-uncased_2_cp.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../input/bertlarge3/bert-large-uncased_3_cp.pt\n",
      "\n",
      "CPU times: user 2min 42s, sys: 1min 57s, total: 4min 39s\n",
      "Wall time: 4min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = QA_TransformerMix('bert-large-uncased', nb_layers=24, pooler_ft=128).cuda()\n",
    "test_dataset_sep_blu = QATestDatasetSep(df_test, model, max_len_q=MAX_LEN, max_len_a=MAX_LEN, max_len_t=MAX_LEN_T, special=True)\n",
    "\n",
    "pred_test = inference(model, test_dataset_sep_blu, '../input/bertlarge3/', sep=True)\n",
    "pred_tests.append(pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 1\n",
    "\n",
    "ENSEMBLING = True\n",
    "\n",
    "CHOSEN = 'wwm2'\n",
    "TO_ENS = ['28_01', 'embedw', 'large1', 'large3', 'wwm3']\n",
    "\n",
    "N_CLUSTER_ENS = [0, 0, 3, 0, 0, 3, 0, 0, 0, 2, 0, 6, 3, 2, 3, 3, 8, 0, 0, 3, 0, 9, 0, 9, 0, 0, 9, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembling 5 models\n"
     ]
    }
   ],
   "source": [
    "pred_test = np.mean(np.array(pred_tests) ** ALPHA, axis=0)\n",
    "\n",
    "if ENSEMBLING:\n",
    "    assert len(pred_tests) == len(TO_ENS)\n",
    "    print(f'Ensembling {len(pred_tests)} models')\n",
    "else:\n",
    "    print(f'Using {CHOSEN} model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "N_CLUSTERS = {\n",
    "    '25_01': [0, 0, 3, 0, 0, 3, 0, 0, 0, 3, 0, 6, 3, 6, 3, 3, 6, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # CV 0.440\n",
    "    '28_01': [0, 0, 3, 0, 0, 3, 0, 0, 0, 2, 0, 6, 3, 3, 3, 3, 8, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # CV 0.4448\n",
    "    '01_02': [0, 0, 6, 0, 9, 3, 0, 3, 0, 2, 0, 7, 3, 2, 3, 3, 8, 0, 0, 3, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0],  # CV 0.4372\n",
    "    'embed' : [0, 0, 3, 0, 0, 3, 0, 0, 0, 3, 0, 6, 3, 2, 3, 3, 7, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # CV 0.4404 - \n",
    "    'special' : [0, 0, 3, 0, 0, 3, 0, 3, 0, 2, 0, 6, 3, 3, 3, 5, 6, 0, 0, 2, 0, 0, 0, 28, 9, 29, 0, 0, 0, 0], # CV 0.4436 - \n",
    "    'mix': [0, 0, 3, 0, 0, 3, 0, 7, 0, 2, 0, 8, 2, 2, 3, 5, 7, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13], # CV 0.4419 - \n",
    "    'wwm' : [0, 0, 5, 0, 0, 3, 0, 8, 3, 2, 0, 7, 3, 4, 4, 3, 8, 0, 0, 6, 0, 0, 0, 0, 3, 0, 8, 0, 0, 0] , # CV 0.4455\n",
    "    'mixw' : [0, 0, 3, 0, 0, 3, 0, 0, 0, 2, 0, 7, 3, 2, 3, 3, 7, 0, 0, 3, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0], # CV 0.4464\n",
    "    'embedw' : [0, 0, 3, 0, 0, 3, 0, 0, 0, 3, 0, 8, 3, 2, 3, 3, 6, 0, 0, 2, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0], # CV 0.4458\n",
    "    'specialw' : [0, 0, 3, 0, 0, 3, 0, 9, 0, 2, 0, 6, 2, 3, 3, 5, 8, 0, 0, 2, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0], # CV 0.444\n",
    "    'doublew': [0, 0, 3, 0, 0, 3, 0, 6, 8, 3, 0, 8, 3, 3, 3, 5, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # CV 0.4401\n",
    "    'large1': [0, 0, 3, 0, 0, 3, 0, 0, 0, 2, 0, 8, 3, 3, 4, 5, 9, 0, 0, 3, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0], # CV 0.4441\n",
    "    'large3': [0, 0, 3, 0, 0, 3, 8, 8, 0, 2, 0, 8, 3, 3, 5, 3, 6, 0, 0, 5, 0, 0, 0, 0, 7, 0, 8, 0, 0, 0], # CV 0.4461\n",
    "    'wwm2': [0, 0, 3, 0, 0, 3, 0, 9, 6, 3, 0, 8, 3, 3, 3, 3, 7, 0, 0, 3, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0], # CV 0.4483\n",
    "    'wwm3': [0, 0, 3, 0, 0, 3, 0, 6, 6, 3, 0, 8, 3, 3, 3, 3, 8, 0, 0, 3, 0, 0, 0, 22, 0, 0, 0, 0, 0, 0], # 0.4525\n",
    "}\n",
    "\n",
    "PRED_OOFS = {\n",
    "    '25_01': '../input/quest-oof/pred_oof2020-01-25_0.406.npy',\n",
    "    '28_01': '../input/quest-oof/pred_oof2020-01-28_0.405.npy',\n",
    "    '01_02': '../input/quest-oof/pred_oof2020-02-01_0.411.npy',\n",
    "    'embed': '../input/quest-oof/pred_oof2020-02-06_0.4089.npy',\n",
    "    'special': '../input/quest-oof/pred_oof2020-02-06_0.4059.npy',\n",
    "    'mix': '../input/quest-oof/pred_oof2020-02-06_0.414.npy',\n",
    "    'wwm' : '../input/quest-oof/pred_oof2020-02-06_0.416-wwm.npy',\n",
    "    'mixw' : '../input/quest-oof/pred_oof2020-02-07_0.414.npy',\n",
    "    'embedw' : '../input/quest-oof/pred_oof2020-02-07_0.4088.npy',\n",
    "    'specialw' : '../input/quest-oof/pred_oof2020-02-08_0.4058.npy',\n",
    "    'doublew': '../input/quest-oof/pred_oof2020-02-07_0.4111.npy',\n",
    "    'large1': '../input/quest-oof/pred_oof2020-02-07_0.416-bl1.npy',\n",
    "    'large3': '../input/quest-oof/pred_oof2020-02-08_0.417-bl3.npy',\n",
    "    'wwm2' : '../input/quest-oof/pred_oof2020-02-08_0.416_wwm_v2.npy',\n",
    "    'wwm3': '../input/questqabertlargev3/pred_oof2020-02-08_0.415.npy'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENSEMBLING:\n",
    "    pred_oof = np.mean(np.array([np.load(PRED_OOFS[date]) for date in TO_ENS]) ** ALPHA, axis=0)\n",
    "else:\n",
    "    pred_oof = np.load(PRED_OOFS[CHOSEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "post_processed_preds = pred_test.copy()\n",
    "\n",
    "for col in range(pred_test.shape[1]):  \n",
    "    if ENSEMBLING:\n",
    "        n_clusts = N_CLUSTER_ENS[col]\n",
    "    else:\n",
    "        n_clusts = N_CLUSTERS[CHOSEN][col]\n",
    "    \n",
    "    if n_clusts:\n",
    "        preds = [0]\n",
    "        while len(np.unique(preds)) == 1: # At least 2 clusters on test data\n",
    "            kmeans = KMeans(n_clusters=n_clusts)\n",
    "\n",
    "        #     kmeans.fit(pred_test[:, col].reshape(-1, 1))\n",
    "            kmeans.fit(np.concatenate([pred_oof, pred_test])[:, col].reshape(-1, 1))\n",
    "            preds = kmeans.cluster_centers_[kmeans.predict(pred_test[:, col].reshape(-1, 1))].reshape(-1)\n",
    "\n",
    "            if len(np.unique(preds)) <= 1:\n",
    "                print(f\"{TARGETS[col]} took only 1 value using {n_clusts} clusters.\")\n",
    "            n_clusts += 1\n",
    "\n",
    "        post_processed_preds[:, col] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "DEBUG_QUESTION_TYPE_SPELLING = False\n",
    "\n",
    "if DEBUG_QUESTION_TYPE_SPELLING:\n",
    "    print('Using working preds for question spelling...')\n",
    "    oof_spelling = np.load(PRED_OOFS['wwm3'])[:, 19]\n",
    "    N_CLUSTS_SPELLING = 6\n",
    "    post_processed_preds = pred_test.copy()\n",
    "\n",
    "\n",
    "    preds_spell = [0]\n",
    "    n_clusts = N_CLUSTS_SPELLING\n",
    "    while len(np.unique(preds_spell)) == 1: # At least 2 clusters on test data\n",
    "        kmeans = KMeans(n_clusters=n_clusts)\n",
    "\n",
    "    #     kmeans.fit(pred_test[:, col].reshape(-1, 1))\n",
    "        kmeans.fit(np.concatenate([oof_spelling, pred_test_spelling]).reshape(-1, 1))\n",
    "        preds_spell = kmeans.cluster_centers_[kmeans.predict(pred_test_spelling.reshape(-1, 1))].reshape(-1)\n",
    "\n",
    "        if len(np.unique(preds)) <= 1:\n",
    "            print(f\"{TARGETS[19]} took only 1 value using {n_clusts} clusters.\")\n",
    "        n_clusts += 1\n",
    "\n",
    "    post_processed_preds[:, 19] = preds_spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0006080824540700975: 475, 0.024543984965188424: 1})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(post_processed_preds[:, 19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processed_preds = (post_processed_preds - post_processed_preds.min(0)) / (post_processed_preds.max(0) - post_processed_preds.min(0) + 1e-2) + 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20024112, 0.27199909, 0.15964718, 0.14436481, 0.17210523,\n",
       "       0.21936884, 0.2082748 , 0.21009018, 0.24164612, 0.19867148,\n",
       "       0.20718873, 0.35191846, 0.15175598, 0.16893079, 0.1365924 ,\n",
       "       0.18239254, 0.38705961, 0.21832731, 0.29400285, 0.03229463,\n",
       "       0.25250739, 0.14896059, 0.17559896, 0.16229467, 0.16685342,\n",
       "       0.16432833, 0.3728473 , 0.19257205, 0.29692411, 0.12792454])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_processed_preds.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.853302</td>\n",
       "      <td>0.477433</td>\n",
       "      <td>0.375735</td>\n",
       "      <td>0.439522</td>\n",
       "      <td>0.350621</td>\n",
       "      <td>0.584233</td>\n",
       "      <td>0.770825</td>\n",
       "      <td>0.672719</td>\n",
       "      <td>0.663801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.852609</td>\n",
       "      <td>0.706211</td>\n",
       "      <td>0.065373</td>\n",
       "      <td>0.710834</td>\n",
       "      <td>0.643477</td>\n",
       "      <td>0.386876</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.862586</td>\n",
       "      <td>0.721979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.529993</td>\n",
       "      <td>0.205285</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.760525</td>\n",
       "      <td>0.629067</td>\n",
       "      <td>0.993384</td>\n",
       "      <td>0.257333</td>\n",
       "      <td>0.188769</td>\n",
       "      <td>0.120047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119693</td>\n",
       "      <td>0.862692</td>\n",
       "      <td>0.418266</td>\n",
       "      <td>0.797313</td>\n",
       "      <td>0.803752</td>\n",
       "      <td>0.657086</td>\n",
       "      <td>0.998684</td>\n",
       "      <td>0.405071</td>\n",
       "      <td>0.083281</td>\n",
       "      <td>0.649691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.712409</td>\n",
       "      <td>0.485875</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.761411</td>\n",
       "      <td>0.821999</td>\n",
       "      <td>0.993384</td>\n",
       "      <td>0.355109</td>\n",
       "      <td>0.213505</td>\n",
       "      <td>0.199167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710335</td>\n",
       "      <td>0.791976</td>\n",
       "      <td>0.195472</td>\n",
       "      <td>0.797313</td>\n",
       "      <td>0.638598</td>\n",
       "      <td>0.494461</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.140911</td>\n",
       "      <td>0.878453</td>\n",
       "      <td>0.732529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.598202</td>\n",
       "      <td>0.124502</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.653458</td>\n",
       "      <td>0.573879</td>\n",
       "      <td>0.993384</td>\n",
       "      <td>0.186443</td>\n",
       "      <td>0.108334</td>\n",
       "      <td>0.094425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342772</td>\n",
       "      <td>0.922520</td>\n",
       "      <td>0.604636</td>\n",
       "      <td>0.873346</td>\n",
       "      <td>0.872145</td>\n",
       "      <td>0.763046</td>\n",
       "      <td>0.908874</td>\n",
       "      <td>0.486404</td>\n",
       "      <td>0.575739</td>\n",
       "      <td>0.673916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.738152</td>\n",
       "      <td>0.086272</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.798367</td>\n",
       "      <td>0.629815</td>\n",
       "      <td>0.993384</td>\n",
       "      <td>0.543789</td>\n",
       "      <td>0.489090</td>\n",
       "      <td>0.162136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087105</td>\n",
       "      <td>0.706211</td>\n",
       "      <td>0.472729</td>\n",
       "      <td>0.797313</td>\n",
       "      <td>0.670121</td>\n",
       "      <td>0.536581</td>\n",
       "      <td>0.392128</td>\n",
       "      <td>0.394999</td>\n",
       "      <td>0.632064</td>\n",
       "      <td>0.618456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0     39                             0.853302                0.477433   \n",
       "1     46                             0.529993                0.205285   \n",
       "2     70                             0.712409                0.485875   \n",
       "3    132                             0.598202                0.124502   \n",
       "4    200                             0.738152                0.086272   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                 0.375735                      0.439522   \n",
       "1                 0.010000                      0.760525   \n",
       "2                 0.010000                      0.761411   \n",
       "3                 0.010000                      0.653458   \n",
       "4                 0.010000                      0.798367   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0               0.350621                               0.584233   \n",
       "1               0.629067                               0.993384   \n",
       "2               0.821999                               0.993384   \n",
       "3               0.573879                               0.993384   \n",
       "4               0.629815                               0.993384   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                         0.770825                       0.672719   \n",
       "1                         0.257333                       0.188769   \n",
       "2                         0.355109                       0.213505   \n",
       "3                         0.186443                       0.108334   \n",
       "4                         0.543789                       0.489090   \n",
       "\n",
       "   question_multi_intent  ...  question_well_written  answer_helpful  \\\n",
       "0               0.663801  ...               0.852609        0.706211   \n",
       "1               0.120047  ...               0.119693        0.862692   \n",
       "2               0.199167  ...               0.710335        0.791976   \n",
       "3               0.094425  ...               0.342772        0.922520   \n",
       "4               0.162136  ...               0.087105        0.706211   \n",
       "\n",
       "   answer_level_of_information  answer_plausible  answer_relevance  \\\n",
       "0                     0.065373          0.710834          0.643477   \n",
       "1                     0.418266          0.797313          0.803752   \n",
       "2                     0.195472          0.797313          0.638598   \n",
       "3                     0.604636          0.873346          0.872145   \n",
       "4                     0.472729          0.797313          0.670121   \n",
       "\n",
       "   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n",
       "0             0.386876                  0.010000               0.077778   \n",
       "1             0.657086                  0.998684               0.405071   \n",
       "2             0.494461                  0.010000               0.140911   \n",
       "3             0.763046                  0.908874               0.486404   \n",
       "4             0.536581                  0.392128               0.394999   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.862586             0.721979  \n",
       "1                        0.083281             0.649691  \n",
       "2                        0.878453             0.732529  \n",
       "3                        0.575739             0.673916  \n",
       "4                        0.632064             0.618456  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.loc[:, 'question_asker_intent_understanding':] = post_processed_preds\n",
    "# sub.loc[:, 'question_asker_intent_understanding':] = pred_test\n",
    "\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 16.9 minutes\n"
     ]
    }
   ],
   "source": [
    "print(f'Done in {(time.time() - START_TIME) / 60 :.1f} minutes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
